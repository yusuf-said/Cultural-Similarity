gnn çıktılarına layernorm koy
not:
Modalitelerin embedding'leri farklı davranıyor olabilir. 
Görsel vektör daha yoğun bilgi taşıyabilir. Transformer bu farkı göremezse, "bu ses neden bu kadar gürültülü" diye kriz yaşar. TransformerFusion ı çok dikkatli yazmalıyız.
attention-weighted topolojik grafikler kullanılabilir görselleştirme için
Transformer’la öğrenilen temsillerin diğer kültürlere mesafesi ölçülebilir.
temporal attention layer ile yıl farklarına bakılabilir.

https://arxiv.org/abs/2208.00339

Her millet için karma değil önce ibadeti, sonra mimarisi sonra başka şeyleri böyle böyle yapılacak. yani tematik şekilde işlnecek.
veya 2. ama yapılabilirse daha iyi yöntem: milletler kümelendirilebilecek kadar eğitilecek ve model oluşturuljp kaydedilecek sonra da fine tune edilecek işlenecek konu kadar.